{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path(os.getenv(\"PROJECT_ROOT\"))\n",
    "DIR_BASE = PROJECT_ROOT / 'data/input/incidents/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(2018, 2023))\n",
    "voivodeships = [\n",
    "    \"dolnośląskie\",\n",
    "    \"kujawsko-pomorskie\",\n",
    "    \"lubelskie\",\n",
    "    \"lubuskie\",\n",
    "    \"mazowieckie\",\n",
    "    \"małopolskie\",\n",
    "    \"opolskie\",\n",
    "    \"podkarpackie\",\n",
    "    \"podlaskie\",\n",
    "    \"pomorskie\",\n",
    "    \"warmińsko-mazurskie\",\n",
    "    \"wielkopolskie\",\n",
    "    \"zachodniopomorskie\",\n",
    "    \"łódzkie\",\n",
    "    \"śląskie\",\n",
    "    \"świętokrzyskie\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_url(voivodeship, year):\n",
    "    base_url = \"https://sewik.pl/search\"\n",
    "    params = {\n",
    "        \"filter_form%5Bvoivodeship%5D\": f\"WOJ.+{voivodeship.upper()}\",\n",
    "        \"filter_form%5BfromDate%5D\": f\"{year}-01-01\",\n",
    "        \"filter_form%5BtoDate%5D\": f\"{year}-12-31\",\n",
    "        \"filter_form%5Bcategories%5D\": \"Czas+zdarze%C5%84\"\n",
    "    }\n",
    "    formatted_params = \"&\".join([f\"{key}={value}\" for key, value in params.items()])\n",
    "    return f\"{base_url}?{formatted_params}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(url):\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        \n",
    "        button = WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.ID, \"filter_form_reports\"))\n",
    "        )\n",
    "        button.click()\n",
    "        \n",
    "        WebDriverWait(driver, 70).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, 'table'))\n",
    "        )\n",
    "        \n",
    "        tables = driver.find_elements(By.TAG_NAME, 'table')\n",
    "        \n",
    "        if tables:\n",
    "            return driver.page_source\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tables_to_csv(html_content, output_dir='csv_output'):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    tables = soup.find_all('table')\n",
    "    if not tables:\n",
    "        print(\"No tables found in the HTML content.\")\n",
    "        return\n",
    "    \n",
    "    for i, table in enumerate(tables, start=1):\n",
    "        h3 = table.find_previous('h3')\n",
    "        if h3:\n",
    "            title = h3.text.strip()\n",
    "            filename = re.sub(r'[^\\w\\s-]', '', title).strip().lower()\n",
    "            filename = re.sub(r'[-\\s]+', '-', filename)\n",
    "        else:\n",
    "            filename = f'table_{i}'\n",
    "        \n",
    "        df = pd.read_html(StringIO(str(table)))[0]\n",
    "        \n",
    "        csv_filename = f'{filename}.csv'\n",
    "        csv_path = os.path.join(output_dir, csv_filename)\n",
    "        \n",
    "        df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"Saved table '{filename}' to {csv_path}\")\n",
    "    \n",
    "    print(f\"Converted {len(tables)} tables to CSV files in {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_case(year, voivodeship):\n",
    "    output_dir = DIR_BASE / str(year) / voivodeship\n",
    "    if os.path.exists(output_dir):\n",
    "        return\n",
    "\n",
    "    url = format_url(voivodeship, year)\n",
    "    html = scrape_page(url)\n",
    "    if not html:\n",
    "        print(f\"Failed to scrape {year}, {voivodeship}\")\n",
    "        return\n",
    "\n",
    "    convert_tables_to_csv(html, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    for voivodeship in voivodeships:\n",
    "        process_case(year, voivodeship)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
